{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reserved-suite",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-array",
   "metadata": {},
   "source": [
    "## Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "helpful-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
       "0   39   77516             13    0          2174             0   \n",
       "1   50   83311             13    0             0             0   \n",
       "2   38  215646              9    0             0             0   \n",
       "3   53  234721              7    0             0             0   \n",
       "4   28  338409             13    1             0             0   \n",
       "\n",
       "   hours-per-week  salary  workclass_Federal-gov  workclass_Local-gov  ...  \\\n",
       "0              40       0                      0                    0  ...   \n",
       "1              13       0                      0                    0  ...   \n",
       "2              40       0                      0                    0  ...   \n",
       "3              40       0                      0                    0  ...   \n",
       "4              40       0                      0                    0  ...   \n",
       "\n",
       "   native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                        0                           0   \n",
       "1                        0                           0   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "4                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             0                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('../data/adult_preprocessed.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-average",
   "metadata": {},
   "source": [
    "## Аномалии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-calibration",
   "metadata": {},
   "source": [
    "Исходя из данных, полученных в файле 1_visualization можно сделать вывод, что аномалии имеются у параметров *hours-peer-week* и *capital-gain*. Проверим, что произойдет с данными, если отбросить значения на расстоянии более 1.5\\*IQR от квартилей Q1 и Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quartile = dataset['hours-per-week'].describe()['25%']\n",
    "third_quartile = dataset['hours-per-week'].describe()['75%']\n",
    "\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "dataset[(dataset['hours-per-week'] > (first_quartile - 3 * iqr)) &\n",
    "            (dataset['hours-per-week'] < (third_quartile + 3 * iqr))].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-medium",
   "metadata": {},
   "source": [
    "Для параметра *hours-per-week* это приводит к потере значительной части данных, кроме того, нельзя исключать, что люди действительно столько работают. Проверим для второго парметра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satisfactory-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 88 columns):\n",
      " #   Column                                     Non-Null Count  Dtype\n",
      "---  ------                                     --------------  -----\n",
      " 0   age                                        0 non-null      int64\n",
      " 1   fnlwgt                                     0 non-null      int64\n",
      " 2   education-num                              0 non-null      int64\n",
      " 3   sex                                        0 non-null      int64\n",
      " 4   capital-gain                               0 non-null      int64\n",
      " 5   capital-loss                               0 non-null      int64\n",
      " 6   hours-per-week                             0 non-null      int64\n",
      " 7   salary                                     0 non-null      int64\n",
      " 8   workclass_Federal-gov                      0 non-null      int64\n",
      " 9   workclass_Local-gov                        0 non-null      int64\n",
      " 10  workclass_Private                          0 non-null      int64\n",
      " 11  workclass_Self-emp-inc                     0 non-null      int64\n",
      " 12  workclass_Self-emp-not-inc                 0 non-null      int64\n",
      " 13  workclass_State-gov                        0 non-null      int64\n",
      " 14  workclass_Without-pay                      0 non-null      int64\n",
      " 15  marital-status_Divorced                    0 non-null      int64\n",
      " 16  marital-status_Married-AF-spouse           0 non-null      int64\n",
      " 17  marital-status_Married-civ-spouse          0 non-null      int64\n",
      " 18  marital-status_Married-spouse-absent       0 non-null      int64\n",
      " 19  marital-status_Never-married               0 non-null      int64\n",
      " 20  marital-status_Separated                   0 non-null      int64\n",
      " 21  marital-status_Widowed                     0 non-null      int64\n",
      " 22  occupation_Adm-clerical                    0 non-null      int64\n",
      " 23  occupation_Armed-Forces                    0 non-null      int64\n",
      " 24  occupation_Craft-repair                    0 non-null      int64\n",
      " 25  occupation_Exec-managerial                 0 non-null      int64\n",
      " 26  occupation_Farming-fishing                 0 non-null      int64\n",
      " 27  occupation_Handlers-cleaners               0 non-null      int64\n",
      " 28  occupation_Machine-op-inspct               0 non-null      int64\n",
      " 29  occupation_Other-service                   0 non-null      int64\n",
      " 30  occupation_Priv-house-serv                 0 non-null      int64\n",
      " 31  occupation_Prof-specialty                  0 non-null      int64\n",
      " 32  occupation_Protective-serv                 0 non-null      int64\n",
      " 33  occupation_Sales                           0 non-null      int64\n",
      " 34  occupation_Tech-support                    0 non-null      int64\n",
      " 35  occupation_Transport-moving                0 non-null      int64\n",
      " 36  relationship_Husband                       0 non-null      int64\n",
      " 37  relationship_Not-in-family                 0 non-null      int64\n",
      " 38  relationship_Other-relative                0 non-null      int64\n",
      " 39  relationship_Own-child                     0 non-null      int64\n",
      " 40  relationship_Unmarried                     0 non-null      int64\n",
      " 41  relationship_Wife                          0 non-null      int64\n",
      " 42  race_Amer-Indian-Eskimo                    0 non-null      int64\n",
      " 43  race_Asian-Pac-Islander                    0 non-null      int64\n",
      " 44  race_Black                                 0 non-null      int64\n",
      " 45  race_Other                                 0 non-null      int64\n",
      " 46  race_White                                 0 non-null      int64\n",
      " 47  native-country_Cambodia                    0 non-null      int64\n",
      " 48  native-country_Canada                      0 non-null      int64\n",
      " 49  native-country_China                       0 non-null      int64\n",
      " 50  native-country_Columbia                    0 non-null      int64\n",
      " 51  native-country_Cuba                        0 non-null      int64\n",
      " 52  native-country_Dominican-Republic          0 non-null      int64\n",
      " 53  native-country_Ecuador                     0 non-null      int64\n",
      " 54  native-country_El-Salvador                 0 non-null      int64\n",
      " 55  native-country_England                     0 non-null      int64\n",
      " 56  native-country_France                      0 non-null      int64\n",
      " 57  native-country_Germany                     0 non-null      int64\n",
      " 58  native-country_Greece                      0 non-null      int64\n",
      " 59  native-country_Guatemala                   0 non-null      int64\n",
      " 60  native-country_Haiti                       0 non-null      int64\n",
      " 61  native-country_Holand-Netherlands          0 non-null      int64\n",
      " 62  native-country_Honduras                    0 non-null      int64\n",
      " 63  native-country_Hong                        0 non-null      int64\n",
      " 64  native-country_Hungary                     0 non-null      int64\n",
      " 65  native-country_India                       0 non-null      int64\n",
      " 66  native-country_Iran                        0 non-null      int64\n",
      " 67  native-country_Ireland                     0 non-null      int64\n",
      " 68  native-country_Italy                       0 non-null      int64\n",
      " 69  native-country_Jamaica                     0 non-null      int64\n",
      " 70  native-country_Japan                       0 non-null      int64\n",
      " 71  native-country_Laos                        0 non-null      int64\n",
      " 72  native-country_Mexico                      0 non-null      int64\n",
      " 73  native-country_Nicaragua                   0 non-null      int64\n",
      " 74  native-country_Outlying-US(Guam-USVI-etc)  0 non-null      int64\n",
      " 75  native-country_Peru                        0 non-null      int64\n",
      " 76  native-country_Philippines                 0 non-null      int64\n",
      " 77  native-country_Poland                      0 non-null      int64\n",
      " 78  native-country_Portugal                    0 non-null      int64\n",
      " 79  native-country_Puerto-Rico                 0 non-null      int64\n",
      " 80  native-country_Scotland                    0 non-null      int64\n",
      " 81  native-country_South                       0 non-null      int64\n",
      " 82  native-country_Taiwan                      0 non-null      int64\n",
      " 83  native-country_Thailand                    0 non-null      int64\n",
      " 84  native-country_Trinadad&Tobago             0 non-null      int64\n",
      " 85  native-country_United-States               0 non-null      int64\n",
      " 86  native-country_Vietnam                     0 non-null      int64\n",
      " 87  native-country_Yugoslavia                  0 non-null      int64\n",
      "dtypes: int64(88)\n",
      "memory usage: 0.0 bytes\n"
     ]
    }
   ],
   "source": [
    "first_quartile = dataset['capital-gain'].describe()['25%']\n",
    "third_quartile = dataset['capital-gain'].describe()['75%']\n",
    "\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "dataset[(dataset['capital-gain'] > (first_quartile - 3 * iqr)) &\n",
    "            (dataset['capital-gain'] < (third_quartile + 3 * iqr))].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-glass",
   "metadata": {},
   "source": [
    "Для второго параметра это приводит к полной потере данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-lobby",
   "metadata": {},
   "source": [
    "## Разбиение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-institution",
   "metadata": {},
   "source": [
    "Для начала разобьем данные на множество описаний объектов и множество меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subsequent-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('salary', 1).values\n",
    "y = dataset['salary'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-swimming",
   "metadata": {},
   "source": [
    "Разобьем на обучающую и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "british-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-remedy",
   "metadata": {},
   "source": [
    "Проверим соотношение разбиения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-street",
   "metadata": {},
   "source": [
    "Нормализуем параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "second-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-gentleman",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-bangladesh",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-karaoke",
   "metadata": {},
   "source": [
    "Произведем обучение и проверим точность классификации алгоритма KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stopped-animal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4118  413]\n",
      " [ 661  841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      4531\n",
      "           1       0.67      0.56      0.61      1502\n",
      "\n",
      "    accuracy                           0.82      6033\n",
      "   macro avg       0.77      0.73      0.75      6033\n",
      "weighted avg       0.81      0.82      0.82      6033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-swimming",
   "metadata": {},
   "source": [
    "Произведем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  clf = KNeighborsClassifier(n_neighbors=5)\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-vancouver",
   "metadata": {},
   "source": [
    "Полученные значения на разных итерациях примерно равны. Можно сделать вывод, что данные устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-intervention",
   "metadata": {},
   "source": [
    "Произведем поиск гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "basic-difficulty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "clf = GridSearchCV(knn, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-prompt",
   "metadata": {},
   "source": [
    "Проверим работу алгоритма с k = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-turtle",
   "metadata": {},
   "source": [
    "Точность работы алгоритма немного увеличилась относительно k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-authority",
   "metadata": {},
   "source": [
    "## DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-column",
   "metadata": {},
   "source": [
    "Произведем обучение и проверим точность классификации алгоритма DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-merchandise",
   "metadata": {},
   "source": [
    "Произведем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  clf = tree.DecisionTreeClassifier()\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-gender",
   "metadata": {},
   "source": [
    "Полученные значения на разных итерациях примерно равны. Можно сделать вывод, что данные устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-british",
   "metadata": {},
   "source": [
    "Произведем поиск гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'max_depth': range(1,11),'max_features': range(4,19)}\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf = GridSearchCV(dtc, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-western",
   "metadata": {},
   "source": [
    "Проверим работу алгоритма с max_depth = 9 и max_features = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 9, max_features = 17)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-browse",
   "metadata": {},
   "source": [
    "Точность ощутимо повысилась"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-modem",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-makeup",
   "metadata": {},
   "source": [
    "Произведем обучение и проверим точность классификации алгоритма NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-acrylic",
   "metadata": {},
   "source": [
    "Произведем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  clf = GaussianNB()\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-electronics",
   "metadata": {},
   "source": [
    "Полученные значения на разных итерациях примерно равны. Можно сделать вывод, что данные устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-nashville",
   "metadata": {},
   "source": [
    "Произведем поиск гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "clf = GridSearchCV(gnb, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-enlargement",
   "metadata": {},
   "source": [
    "Проверим работу алгоритма с var_smoothing = 0.0657933224657568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = GaussianNB(var_smoothing = 0.0657933224657568)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-television",
   "metadata": {},
   "source": [
    "Точность примерна равна точности со значением по умолчанию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-perspective",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-paint",
   "metadata": {},
   "source": [
    "Произведем обучение и проверим точность классификации алгоритма SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qualified-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4214  317]\n",
      " [ 633  869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90      4531\n",
      "           1       0.73      0.58      0.65      1502\n",
      "\n",
      "    accuracy                           0.84      6033\n",
      "   macro avg       0.80      0.75      0.77      6033\n",
      "weighted avg       0.84      0.84      0.84      6033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-platform",
   "metadata": {},
   "source": [
    "Произведем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  clf = svm.SVC()\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-smile",
   "metadata": {},
   "source": [
    "Полученные значения на разных итерациях примерно равны. Можно сделать вывод, что данные устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-duplicate",
   "metadata": {},
   "source": [
    "Произведем поиск гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'C': [0.1,1, 10, 100]}\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-republic",
   "metadata": {},
   "source": [
    "Проверим работу алгоритма с C = , gamma = , kernel = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-amount",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "announced-leeds",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-saturday",
   "metadata": {},
   "source": [
    "Произведем обучение и проверим точность классификации алгоритма LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-digest",
   "metadata": {},
   "source": [
    "Произведем кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  clf = LogisticRegression()\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-illustration",
   "metadata": {},
   "source": [
    "Полученные значения на разных итерациях примерно равны. Можно сделать вывод, что данные устойчивы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-neutral",
   "metadata": {},
   "source": [
    "Произведем поиск гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'C': np.logspace(-4, 4, 50), 'penalty': ['l1', 'l2']}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "clf = GridSearchCV(lr, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-anaheim",
   "metadata": {},
   "source": [
    "Проверим работу алгоритма с C = , penalty = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-wellington",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "spare-interface",
   "metadata": {},
   "source": [
    "## Выводы по работе алгоритмов на данной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-notebook",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
